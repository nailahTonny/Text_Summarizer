# -*- coding: utf-8 -*-
"""AITextSummarizer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m4HkapYVBoG_wzoXJWboeFHrmkXbNiTg
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit gtts python-dotenv

import os
import json
import requests
import streamlit as st
from gtts import gTTS
from textblob import TextBlob
from collections import Counter
import string
import tempfile
import matplotlib.pyplot as plt
from dotenv import load_dotenv

# 🔐 Secure API key loading: .env or Colab environment variables
import os


GROQ_API_KEY = os.getenv("GROQ_API_KEY")

if not GROQ_API_KEY:
    st.error("❌ GROQ_API_KEY not found. Please set it as a Colab environment variable.")
    st.stop()


# ---------- Groq LLaMA3-8B Summarizer ----------
def generate_summary(text):
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "llama3-8b-8192",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant that summarizes conversations."},
            {"role": "user", "content": f"Summarize this chat:\n{text}"}
        ]
    }
    response = requests.post("https://api.groq.com/openai/v1/chat/completions", headers=headers, json=data)
    result = response.json()
    return result['choices'][0]['message']['content']

# ---------- Text-to-Speech ----------
def speak_summary(text):
    tts = gTTS(text=text, lang='en')
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as fp:
        tts.save(fp.name)
        return fp.name

# ---------- Sentiment Timeline ----------
def get_sentiment_timeline(text):
    lines = [line for line in text.split("\n") if len(line.strip()) > 10]
    sentiments = [TextBlob(line).sentiment.polarity for line in lines]
    return sentiments

# ---------- Text Stats ----------
def analyze_text_stats(text):
    lines = text.splitlines()
    words = text.split()
    cleaned_words = [w.strip(string.punctuation).lower() for w in words if len(w) > 3]
    word_freq = Counter(cleaned_words)
    most_common_words = word_freq.most_common(5)

    return {
        "Total Words": len(words),
        "Longest Line": max(lines, key=len) if lines else '',
        "Important Words": [word for word, _ in most_common_words]
    }



# ---------- Streamlit UI ----------
st.set_page_config(page_title="Text Summarizer", layout="wide")
st.title("🤖 Text Summarizer")

st.header("📤 Upload a Chat Text File")
uploaded_file = st.file_uploader("Choose a .txt file", type="txt")

if uploaded_file:
    with st.spinner("Processing file and generating summary..."):
        text = uploaded_file.read().decode("utf-8")
        summary = generate_summary(text)
        sentiment_scores = get_sentiment_timeline(text)
        stats = analyze_text_stats(text)

    st.success("✅ File processed and summary generated!")

    # Summary Section
    st.header("📄 Summarized Text")
    st.write(summary)

    if st.button("🔊 Listen to Summary"):
        audio_path = speak_summary(summary)
        st.audio(open(audio_path, 'rb').read(), format='audio/mp3')

    # Sentiment Chart Section
    st.header("📈 Sentiment Analysis Timeline")

    if sentiment_scores:
        # Display legend
        st.markdown("""
        **Sentiment Legend:**
        🟩 Positive  🟨 Neutral  🟥 Negative
        """)

        # Create color-coded line chart
        fig, ax = plt.subplots(figsize=(8, 2.8))

        ax.plot(sentiment_scores, color="#1f77b4", linewidth=2, marker='o', markersize=4)

        # Highlight thresholds
        ax.axhline(0.1, color='green', linestyle='--', linewidth=1)
        ax.axhline(-0.1, color='red', linestyle='--', linewidth=1)
        ax.axhline(0, color='gray', linestyle=':', linewidth=0.8)

        ax.set_title("Sentiment Polarity Across Lines", fontsize=13)
        ax.set_xlabel("Line Index")
        ax.set_ylabel("Polarity")

        st.pyplot(fig)
    else:
        st.info("Not enough content to generate sentiment timeline.")


    # Text Stats Section
    st.header("📊 Text Insights")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown(f"""
            <div style="background-color:#E3F2FD;padding:20px;border-radius:10px;text-align:center">
                <h4 style="color:#0D47A1">Total Words</h4>
                <h2 style="color:#1565C0">{stats['Total Words']}</h2>
            </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown(f"""
            <div style="background-color:#F1F8E9;padding:20px;border-radius:10px;text-align:center">
                <h4 style="color:#33691E">Longest Line</h4>
                <p style="color:#558B2F">{stats['Longest Line']}</p>
            </div>
        """, unsafe_allow_html=True)

    with col3:
        st.markdown(f"""
            <div style="background-color:#FFF3E0;padding:20px;border-radius:10px;text-align:center">
                <h4 style="color:#E65100">Important Words</h4>
                <h3 style="color:#F57C00">{', '.join(stats['Important Words'])}</h3>
            </div>
        """, unsafe_allow_html=True)

else:
    st.warning("Please upload a .txt file to get started.")